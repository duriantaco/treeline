================================================================================
CHUNK 4 OF 5
================================================================================

========================================
File: ./optimization/indexer.py
========================================
# flake8: noqa: E203
import ast
import json
import mmap
from collections import defaultdict
from concurrent.futures import ProcessPoolExecutor
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Set

import xxhash


@dataclass
class IndexEntry:
    """Represents a single indexed code element"""

    path: str
    line_start: int
    line_end: int
    type: str  # 'class', 'function', 'import'
    name: str
    hash: str
    dependencies: Set[str]


class FastIndexer:
    def __init__(self, max_workers: int = 8):
        self.index: Dict[str, IndexEntry] = {}
        self.dependency_graph = defaultdict(set)
        self.reverse_index = defaultdict(set)
        self.file_hashes = {}
        self.max_workers = max_workers

        self.last_index_state: Dict[str, str] = {}

        self.index_state_file = "treeline_index_state.json"

        self._load_index_state()

    def _load_index_state(self):
        index_state_path = Path(self.index_state_file)
        if index_state_path.exists():
            try:
                with open(index_state_path, "r", encoding="utf-8") as f:
                    self.last_index_state = json.load(f)
            except Exception as e:
                print(f"Could not load index state file: {e}")

    def _save_index_state(self):
        try:
            with open(self.index_state_file, "w", encoding="utf-8") as f:
                json.dump(self.file_hashes, f, indent=2)
        except Exception as e:
            print(f"Could not save index state: {e}")

    def _fast_hash_file(self, file_path: Path) -> str:
        """Use xxhash for extremely fast file hashing"""
        hasher = xxhash.xxh64()

        chunk_size = 2_000_000  # 2 MB at a time
        with open(file_path, "rb") as f:
            while True:
                chunk = f.read(chunk_size)
                if not chunk:
                    break
                hasher.update(chunk)

        return hasher.hexdigest()

    def _parse_file_fast(self, file_path: Path) -> List[IndexEntry]:
        """Full AST parsing for code elements (function/class/import)"""
        entries = []
        try:
            with open(file_path, "rb") as f:
                with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
                    content = mm.read().decode("utf-8")
                    tree = ast.parse(content)

                    for node in ast.walk(tree):
                        if isinstance(node, (ast.FunctionDef, ast.ClassDef)):
                            entry = IndexEntry(
                                path=str(file_path),
                                line_start=node.lineno,
                                line_end=node.end_lineno,
                                type=(
                                    "function"
                                    if isinstance(node, ast.FunctionDef)
                                    else "class"
                                ),
                                name=node.name,
                                hash=xxhash.xxh64(
                                    content[node.lineno : node.end_lineno]
                                ).hexdigest(),
                                dependencies=set(),
                            )
                            entries.append(entry)

                        elif isinstance(node, (ast.Import, ast.ImportFrom)):
                            if entries:
                                if isinstance(node, ast.Import):
                                    for alias in node.names:
                                        entries[-1].dependencies.add(alias.name)
                                else:
                                    module = node.module
                                    for alias in node.names:
                                        full_name = f"{module}.{alias.name}"
                                        entries[-1].dependencies.add(full_name)

        except Exception as e:
            print(f"Error parsing {file_path} in _parse_file_fast: {e}")
            return []

        return entries

    def index_codebase(self, root_path: Path):
        """
        Index entire codebase using parallel processing:
          1) Hash all files in parallel
          2) Parse only those that changed since last run
          3) Build up our dependency graph
        """

        python_files = list(root_path.rglob("*.py"))
        total_files = len(python_files)

        if total_files == 0:
            print("No Python files found to index.")
            return

        print(f"Found {total_files} Python files. Hashing...")

        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_path = {
                executor.submit(self._fast_hash_file, path): path
                for path in python_files
            }

            for i, future in enumerate(future_to_path, start=1):
                path = future_to_path[future]
                try:
                    file_hash = future.result()
                    self.file_hashes[str(path)] = file_hash
                except Exception as e:
                    print(f"Error hashing {path}: {e}")

                if i % 100 == 0:
                    print(f" Hashed {i}/{total_files} files...")

        self._save_index_state()

        print("Hashing completed. Checking which files changed...")

        # if hash is the same as the last index run, skip re-parsing
        files_to_parse = []
        for path in python_files:
            path_str = str(path)
            new_hash = self.file_hashes.get(path_str, "")
            old_hash = self.last_index_state.get(path_str, "")

            if old_hash != new_hash:
                # file has changed or wasn't indexed before
                files_to_parse.append(path)

        changed_count = len(files_to_parse)
        print(f"{changed_count} files changed or new since last index; parsing now...")

        if changed_count == 0:
            print("No files changed. Skipping AST parsing.")
            return

        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_path = {
                executor.submit(self._parse_file_fast, path): path
                for path in files_to_parse
            }

            for i, future in enumerate(future_to_path, start=1):
                path = future_to_path[future]
                try:
                    entries = future.result()
                    for entry in entries:
                        self.index[entry.hash] = entry

                        # update the dependency graph with new dependencies
                        for dep in entry.dependencies:
                            self.dependency_graph[entry.name].add(dep)
                            self.reverse_index[dep].add(entry.name)

                except Exception as e:
                    print(f"Error parsing {path}: {e}")

                if i % 50 == 0:
                    print(f" Parsed {i}/{changed_count} changed files...")

        self._save_index_state()
        print("Indexing completed.")

    def get_dependencies(self, name: str, depth: int = -1) -> Set[str]:
        """Get all dependencies for a given element, with optional depth limit"""
        if depth == 0:
            return set()
        deps = self.dependency_graph[name]
        if depth == -1:
            for dep in list(deps):
                deps.update(self.get_dependencies(dep))
        else:
            for dep in list(deps):
                deps.update(self.get_dependencies(dep, depth - 1))
        return deps

    def get_dependents(self, name: str, depth: int = -1) -> Set[str]:
        """Get all elements that depend on the given element"""
        if depth == 0:
            return set()
        deps = self.reverse_index[name]
        if depth == -1:
            for dep in list(deps):
                deps.update(self.get_dependents(dep))
        else:
            for dep in list(deps):
                deps.update(self.get_dependents(dep, depth - 1))
        return deps

    def get_definitions_for_file(self, file_path: str) -> List[IndexEntry]:
        """
        Returns a list of IndexEntry objects that belong to the given file path.

        This simulates an LSP call that fetches definitions for an editor.
        """
        results = []
        for entry_hash, entry in self.index.items():
            # compare the entry's path to the requested file_path
            if entry.path == file_path:
                results.append(entry)
        return results

========================================
File: ./utils/report.py
========================================
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set, Tuple, Union

class ReportGenerator:
    def __init__(self, target_dir: Path, output_dir: Path = None):
        self.target_dir = target_dir
        self.output_dir = output_dir or Path("treeline_reports")
        self.output_dir.mkdir(exist_ok=True)

        from treeline.dependency_analyzer import ModuleDependencyAnalyzer
        from treeline.enhanced_analyzer import EnhancedCodeAnalyzer

        self.dependency_analyzer = ModuleDependencyAnalyzer()
        self.enhanced_analyzer = EnhancedCodeAnalyzer()

        self.analyzed_files = []
        self.quality_issues = defaultdict(list)
        self.complexity_metrics = {}
        self.dependency_graph = {}
        self.entry_points = []
        self.core_components = []
        self.code_smells_by_category = defaultdict(list)
        self.issues_by_file = defaultdict(list)
        self.issues_count = 0

    def analyze(self):
        print(f"Analyzing {self.target_dir}...")

        for py_file in self.target_dir.rglob("*.py"):
            if not self._should_analyze_file(py_file):
                continue

            self.analyzed_files.append(py_file)
            try:
                self.enhanced_analyzer.analyze_file(py_file)
            except Exception as e:
                print(f"Error analyzing {py_file}: {e}")

        self.quality_issues = self.enhanced_analyzer.quality_issues
        self.issues_count = sum(len(issues) for issues in self.quality_issues.values())

        for category, issues in self.quality_issues.items():
            for issue in issues:
                if isinstance(issue, dict) and 'file_path' in issue:
                    file_path = issue['file_path']
                    self.issues_by_file[file_path].append({
                        'category': category,
                        'description': issue.get('description', 'Unknown issue'),
                        'line': issue.get('line', 'Unknown')
                    })
                    self.code_smells_by_category[category].append(issue)

        self.dependency_analyzer.analyze_directory(self.target_dir)
        self.entry_points = self.dependency_analyzer.get_entry_points()
        self.core_components = self.dependency_analyzer.get_core_components()
        
        self.complexity_metrics = {
            'complex_functions': self.dependency_analyzer.complex_functions,
            'thresholds': self.dependency_analyzer.QUALITY_METRICS
        }

        print(f"Analysis complete! Found {self.issues_count} issues in {len(self.analyzed_files)} files.")

    def generate_report(self) -> str:
        sections = []
        
        sections.append(f"# Treeline Code Analysis Report\n")
        sections.append(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        sections.append(f"**Project:** {self.target_dir.absolute()}")
        sections.append(f"**Files Analyzed:** {len(self.analyzed_files)}")
        sections.append(f"**Issues Found:** {self.issues_count}\n")
        
        sections.append("## Quality Issues Overview\n")
        
        if self.issues_count > 0:
            sections.append("| Category | Count |")
            sections.append("| --- | ---: |")
            for category, issues in self.code_smells_by_category.items():
                sections.append(f"| {category.title()} | {len(issues)} |")
        else:
            sections.append("No quality issues found. Great job!")
        
        security_issues = self.code_smells_by_category.get('security', [])
        sql_injection_issues = [
            issue for category, issues in self.code_smells_by_category.items()
            for issue in issues
            if isinstance(issue, dict) and 
            issue.get('description', '').lower().find('sql injection') != -1
        ]
        
        if security_issues or sql_injection_issues:
            sections.append("\n## Security Issues\n")
            
            if security_issues:
                sections.append("### General Security Issues\n")
                for issue in security_issues:
                    if isinstance(issue, dict):
                        file_path = issue.get('file_path', 'Unknown')
                        rel_path = Path(file_path).relative_to(self.target_dir) if Path(file_path).is_absolute() else file_path
                        line = issue.get('line', 'Unknown')
                        description = issue.get('description', 'Unknown issue')
                        
                        sections.append(f"- **{rel_path}:{line}**: {description}")
            
            if sql_injection_issues:
                sections.append("\n### SQL Injection Vulnerabilities\n")
                for issue in sql_injection_issues:
                    if isinstance(issue, dict):
                        file_path = issue.get('file_path', 'Unknown')
                        rel_path = Path(file_path).relative_to(self.target_dir) if Path(file_path).is_absolute() else file_path
                        line = issue.get('line', 'Unknown')
                        description = issue.get('description', 'Unknown issue')
                        
                        sections.append(f"- **{rel_path}:{line}**: {description}")
        
        sections.append("\n## Complexity Hotspots\n")
        
        if self.complexity_metrics['complex_functions']:
            sections.append("| Module | Function | Complexity |")
            sections.append("| --- | --- | ---: |")
            
            threshold = self.complexity_metrics['thresholds']['MAX_CYCLOMATIC_COMPLEXITY']
            
            for module, func, complexity in sorted(
                self.complexity_metrics['complex_functions'], 
                key=lambda x: x[2], 
                reverse=True
            )[:10]:
                sections.append(f"| {module} | {func} | {complexity} |")
                
            if len(self.complexity_metrics['complex_functions']) > 10:
                sections.append(f"\n*...and {len(self.complexity_metrics['complex_functions']) - 10} more complex functions*")
        else:
            sections.append("No complex functions found. Great job!")
            
        sections.append("\n## Top Issues by File\n")
        
        if self.issues_by_file:
            top_files = sorted(
                self.issues_by_file.items(), 
                key=lambda x: len(x[1]), 
                reverse=True
            )[:5]
            
            for file_path, issues in top_files:
                rel_path = Path(file_path).relative_to(self.target_dir) if Path(file_path).is_absolute() else file_path
                sections.append(f"### {rel_path} ({len(issues)} issues)\n")
                
                issues_by_category = defaultdict(list)
                for issue in issues:
                    issues_by_category[issue['category']].append(issue)
                    
                for category, cat_issues in issues_by_category.items():
                    sections.append(f"**{category.title()}** ({len(cat_issues)})\n")
                    
                    for issue in cat_issues[:3]:
                        line_info = f" (Line {issue['line']})" if issue['line'] != 'Unknown' else ""
                        sections.append(f"- {issue['description']}{line_info}")
                    
                    if len(cat_issues) > 3:
                        sections.append(f"- *...and {len(cat_issues) - 3} more {category} issues*")
                    
                    sections.append("")
                    
            if len(self.issues_by_file) > 5:
                sections.append(f"\n*...and {len(self.issues_by_file) - 5} more files with issues*")
        else:
            sections.append("No quality issues found. Great job!")
            
        sections.append("\n## Core Components\n")
        
        if self.core_components:
            sections.append("| Component | Incoming | Outgoing |")
            sections.append("| --- | ---: | ---: |")
            for comp in self.core_components:
                sections.append(f"| {comp['name']} | {comp['incoming']} | {comp['outgoing']} |")
        else:
            sections.append("No core components identified.")
            
        sections.append("\n## Entry Points\n")
        
        if self.entry_points:
            for ep in self.entry_points:
                sections.append(f"- {ep}")
        else:
            sections.append("No entry points identified.")
        
        sections.append("\n---\n")
        sections.append("Report generated by Treeline")
        
        return "\n".join(sections)

    def save_report(self, filename: str = None) -> Path:
        content = self.generate_report()
        
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"treeline_report_{timestamp}.md"
        
        output_path = self.output_dir / filename
        
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(content)
            
        print(f"Report saved to {output_path}")
        return output_path

    def _should_analyze_file(self, file_path: Path) -> bool:
        if any(p in str(file_path) for p in ["venv", "site-packages", "__pycache__", ".git"]):
            return False
            
        return True

========================================
File: ./cli.py
========================================
#!/usr/bin/env python3
import json
from pathlib import Path

import click
from rich.console import Console
from rich.panel import Panel
from rich.table import Table

from treeline.dependency_analyzer import ModuleDependencyAnalyzer
from treeline.enhanced_analyzer import EnhancedCodeAnalyzer
from treeline.utils.report import ReportGenerator

console = Console()


@click.group()
def cli():
    """
    🌲 Treeline - Code Analysis & Visualization Tool

    A CLI to analyze and visualize Python codebases. Provides commands to:
    - Analyze structural dependencies
    - Check code quality
    - Generate reports
    - Launch a web interface
    """
    pass


@cli.command()
@click.argument("directory", type=click.Path(exists=True))
@click.option("--depth", default=1, help="Analysis depth for dependencies")
def analyze(directory, depth):
    """
    Analyze your codebase structure and quality metrics.

    \b
    Examples:
      treeline analyze /path/to/codebase
      treeline analyze . --depth 2
    """
    with console.status("[bold green]Analyzing codebase..."):
        try:
            dep_analyzer = ModuleDependencyAnalyzer()
            code_analyzer = EnhancedCodeAnalyzer()

            dep_analyzer.analyze_directory(Path(directory))

            entry_points = dep_analyzer.get_entry_points()
            core_components = dep_analyzer.get_core_components()

            console.print("\n[bold]📊 Analysis Results[/]")

            console.print("\n[bold]Entry Points:[/]")
            if entry_points:
                for ep in entry_points:
                    console.print(f"  • {ep}")
            else:
                console.print("  (None found)")

            console.print("\n[bold]Core Components:[/]")
            if core_components:
                table = Table(show_header=True)
                table.add_column("Component")
                table.add_column("Incoming", justify="right")
                table.add_column("Outgoing", justify="right")
                for comp in core_components:
                    table.add_row(
                        comp["name"], 
                        str(comp["incoming"]), 
                        str(comp["outgoing"])
                    )
                console.print(table)
            else:
                console.print("  (No core components identified)")

            console.print(
                "\n[green]Analysis complete![/] You can now explore more details."
            )

        except Exception as e:
            console.print(
                f"\n[red]Error during analysis:[/] {str(e)}",
                style="bold red"
            )


@cli.command()
@click.argument("directory", type=click.Path(exists=True))
@click.option("--min-complexity", default=10, help="Minimum complexity to report")
def quality(directory, min_complexity):
    """
    Analyze code quality metrics and highlight complex or smelly code.

    \b
    Examples:
      treeline quality /path/to/codebase
      treeline quality . --min-complexity 12
    """
    with console.status("[bold green]Analyzing code quality..."):
        try:
            analyzer = EnhancedCodeAnalyzer()
            results = []

            for file in Path(directory).rglob("*.py"):
                results.extend(analyzer.analyze_file(file))

            console.print("\n[bold]🔍 Code Quality Report[/]\n")

            complex_funcs = [
                r for r in results
                if r["type"] == "function"
                and r["metrics"]["complexity"] >= min_complexity
            ]
            if complex_funcs:
                table = Table(show_header=True, title="Complex Functions")
                table.add_column("Function")
                table.add_column("Complexity", justify="right")
                table.add_column("Lines", justify="right")

                for func in sorted(
                    complex_funcs,
                    key=lambda x: x["metrics"]["complexity"],
                    reverse=True,
                ):
                    table.add_row(
                        func["name"],
                        str(func["metrics"]["complexity"]),
                        str(func["metrics"]["lines"]),
                    )
                console.print(Panel(table))
            else:
                console.print("No functions exceed the specified complexity threshold.")

            smells = [r for r in results if r["code_smells"]]
            if smells:
                console.print("\n[bold]Code Smells:[/]")
                for item in smells:
                    console.print(f"\n• [bold]{item['type']}:[/] {item['name']}")
                    for smell in item["code_smells"]:
                        console.print(f"   - {smell}")
            else:
                console.print("\nNo code smells detected. Great job!")

        except Exception as e:
            console.print(f"[red]Error:[/] {str(e)}", style="bold red")


@cli.command()
def serve():
    """
    Start the Treeline web interface using FastAPI + Uvicorn.

    \b
    Examples:
      treeline serve
    """
    try:
        import uvicorn
        from treeline.api.app import app

        console.print("[green]Starting Treeline web interface...[/]")
        console.print("Visit http://localhost:8000 in your browser")

        uvicorn.run(app, host="0.0.0.0", port=8000)
    except ImportError:
        console.print("[red]Error:[/] Missing required packages. Install with:")
        console.print("  pip install fastapi uvicorn")
    except Exception as e:
        console.print(f"[red]Error starting web interface:[/] {str(e)}", style="bold red")


@cli.command()
@click.argument("directory", type=click.Path(exists=True), default=".")
@click.option("--output", default=None, help="Output markdown filename (default: treeline_report.md)")
def report(directory, output):
    """
    Generate a markdown report summarizing analysis results.

    \b
    Examples:
      treeline report /path/to/codebase
      treeline report . --output custom_report.md
    """
    with console.status("[bold green]Generating markdown report..."):
        try:
            report_gen = ReportGenerator(Path(directory))
            report_gen.analyze()

            filename = output or "treeline_report.md"
            report_path = report_gen.save_report(filename=filename)

            console.print(f"\n[green]Markdown report saved to {report_path}[/]")
        except Exception as e:
            console.print(f"\n[red]Error during report generation:[/] {str(e)}", style="bold red")

if __name__ == "__main__":
    cli()

========================================
File: ./models/dependency_analyzer.py
========================================
from dataclasses import dataclass

from ..type_checker import TypeChecked


@dataclass
class FunctionLocation(TypeChecked):
    module: str
    file: str
    line: int


@dataclass
class FunctionCallInfo(TypeChecked):
    from_module: str
    from_function: str
    line: int


@dataclass
class ClassMethod(TypeChecked):
    line: int
    calls: list[str]


@dataclass
class ClassInfo(TypeChecked):
    module: str
    file: str
    line: int
    methods: dict[str, ClassMethod]


@dataclass
class ModuleMetrics(TypeChecked):
    functions: int
    classes: int
    complexity: int


@dataclass
class ComplexFunction(TypeChecked):
    module: str
    name: str
    complexity: int


@dataclass
class MethodInfo(TypeChecked):
    line: int
    calls: list[str]


@dataclass
class Node:
    id: int
    name: str
    type: str
    metrics: dict = None
    code_smells: list = None

    def __post_init__(self):
        if self.metrics is None:
            self.metrics = {}
        if self.code_smells is None:
            self.code_smells = []


@dataclass
class Link:
    source: int
    target: int
    type: str


@dataclass
class GraphData(TypeChecked):
    nodes: list[Node]
    links: list[Link]

========================================
File: ./checkers/code_smells.py
========================================
import ast
from collections import defaultdict
from pathlib import Path
from typing import Dict

from treeline.models.enhanced_analyzer import QualityIssue

class CodeSmellChecker:
    def __init__(self, config: Dict = None):
        self.config = config or {}
        self.max_params = self.config.get("MAX_PARAMS", 5)

    def check(self, tree: ast.AST, file_path: Path, quality_issues: defaultdict):
        self._check_magic_numbers(tree, file_path, quality_issues)
        self._check_long_parameter_lists(tree, file_path, quality_issues)

    def _check_magic_numbers(self, tree: ast.AST, file_path: Path, quality_issues: defaultdict):
        for node in ast.walk(tree):
            if isinstance(node, ast.Constant) and isinstance(node.value, (int, float)):
                quality_issues["code_smells"].append(QualityIssue(
                    description="Magic number detected",
                    file_path=str(file_path),
                    line=node.lineno
                ).__dict__)

    def _check_long_parameter_lists(self, tree: ast.AST, file_path: Path, quality_issues: defaultdict):
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef) and len(node.args.args) > self.max_params:
                quality_issues["code_smells"].append(QualityIssue(
                    description=f"Function has too many parameters (>{self.max_params})",
                    file_path=str(file_path),
                    line=node.lineno
                ).__dict__)
========================================
File: ./checkers/security.py
========================================
import ast
from collections import defaultdict
from pathlib import Path
from typing import Dict

from treeline.models.enhanced_analyzer import QualityIssue

class SecurityAnalyzer:
    def __init__(self, config: Dict = None):
        self.config = config or {}

    def check(self, tree: ast.AST, file_path: Path, quality_issues: defaultdict):
        self._check_hardcoded_credentials(tree, file_path, quality_issues)

    def _check_hardcoded_credentials(self, tree: ast.AST, file_path: Path, quality_issues: defaultdict):
        for node in ast.walk(tree):
            if isinstance(node, ast.Constant) and isinstance(node.value, str) and any(kw in node.value.lower() for kw in ["password", "key", "secret"]):
                quality_issues["security"].append(QualityIssue(
                    description="Possible hardcoded credential",
                    file_path=str(file_path),
                    line=node.lineno
                ).__dict__)
========================================
File: ./core.py
========================================
# treeline/treeline/core.py

from pathlib import Path
from treeline.cli import cli

def create_default_ignore():
    """Create a default .treeline-ignore file if it does not already exist."""
    ignore_file = Path(".treeline-ignore")
    if not ignore_file.exists():
        ignore_file.write_text(
            "\n".join([
                "*.pyc",
                "__pycache__/",
                ".git/",
                ".env/",
                "venv/",
                ".venv/",
                ".DS_Store",
                "node_modules/",
                "env/",
                "build/",
                "dist/",
                ""
            ])
        )
        print("Created .treeline-ignore file")


def main():
    """Entry point for the Treeline CLI."""
    create_default_ignore() 
    cli()  


if __name__ == "__main__":
    main()

